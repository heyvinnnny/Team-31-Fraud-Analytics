{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from time import sleep\n",
    "import csv\n",
    "\n",
    "def scrape_jobs(search_term):\n",
    "    columns = [\n",
    "        'title',\n",
    "        'company',\n",
    "        'job_post_id',\n",
    "        'address',\n",
    "        'employment_type',\n",
    "        'seniority',\n",
    "        'min_experience',\n",
    "        'job_category',\n",
    "        'salary',\n",
    "        'salary_type',\n",
    "        'num_of_applications',\n",
    "        'last_posted_date',\n",
    "        'expiry_date',\n",
    "        'description',\n",
    "        'company_info'\n",
    "    ]\n",
    "    jobs = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Set up Chrome WebDriver *Important to have!!\n",
    "    chrome_options = ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome('/Users/kevin/Downloads/chromedriver-win64/chromedriver-win64/chromedriver')\n",
    "\n",
    "    count = -1\n",
    "    # Loop through the pages\n",
    "    for page in range(1, 10):  # Adjust the range according to the number of pages you want to scrape\n",
    "        url = f'https://www.mycareersfuture.gov.sg/search?search={search_term}&sortBy=new_posting_date&page={page}'\n",
    "        driver.get(url)\n",
    "        assert 'MyCareersFuture' in driver.title\n",
    "        sleep(6)\n",
    "\n",
    "        \n",
    "        for i in range(20):\n",
    "            try:\n",
    "                # Extract information from the job card\n",
    "                card_elem = driver.find_element(By.XPATH, f\"(//a[@data-testid='job-card-link'])[{i + 1}]\")\n",
    "                title = card_elem.find_element(By.XPATH, \".//span[contains(@class, 'JobCard__jobtitle')]\").text.strip()\n",
    "                loc = card_elem.find_element(By.XPATH, \".//p[@data-cy='job-card__location']\").text.strip()\n",
    "\n",
    "                # Enter into job details\n",
    "                card_elem.click()\n",
    "                sleep(8)\n",
    "                \n",
    "                # Indicator\n",
    "                print(f\"Scraping page {page} - Job {i + 1}\")\n",
    "\n",
    "                # Extract job details from the job details section\n",
    "                job_details_elem = driver.find_element(By.XPATH, \"//div[@data-cy='JobDetails__job-info']\")\n",
    "                print(job_details_elem)\n",
    "                title = job_details_elem.find_element(By.XPATH, \".//h1[@id='job_title']\").text.strip()\n",
    "                company = job_details_elem.find_element(By.XPATH, \".//p[@data-cy='company-hire-info__company']\").text.strip()\n",
    "                job_post_id = job_details_elem.find_element(By.XPATH, \".//span[@data-cy='jobinfo__jobpostid--span']\").text.strip()\n",
    "\n",
    "                # Error exception\n",
    "                try:\n",
    "                    address_elem = job_details_elem.find_element(By.XPATH, \".//p[@id='address']/a\")\n",
    "                    address = address_elem.text.strip()\n",
    "                except:\n",
    "                    address = \"\"\n",
    "\n",
    "                employment_type = job_details_elem.find_element(By.XPATH, \".//p[@id='employment_type']\").text.strip()\n",
    "                seniority = job_details_elem.find_element(By.XPATH, \".//p[@id='seniority']\").text.strip()\n",
    "                \n",
    "                try:\n",
    "                    min_experience = job_details_elem.find_element(By.XPATH, \".//p[@id='min_experience']\").text.strip()\n",
    "                except:\n",
    "                    min_experience = \"\"\n",
    "                    \n",
    "                job_category = job_details_elem.find_element(By.XPATH, \".//p[@id='job-categories']\").text.strip()\n",
    "                salary_range_elem = job_details_elem.find_element(By.XPATH, \".//div[@class='lh-solid']\")\n",
    "                salary = salary_range_elem.text.strip().split(' to ')\n",
    "                salary_type = job_details_elem.find_element(By.XPATH, \".//span[@data-cy='salary-type']\").text.strip()\n",
    "                num_of_applications = job_details_elem.find_element(By.XPATH, \".//span[@id='num_of_applications']\").text.strip()\n",
    "                last_posted_date = job_details_elem.find_element(By.XPATH, \".//span[@id='last_posted_date']\").text.strip()\n",
    "                expiry_date = job_details_elem.find_element(By.XPATH, \".//span[@id='expiry_date']\").text.strip()\n",
    "\n",
    "                # Extract description (roles & responsibilities)\n",
    "                description_elem = job_details_elem.find_element(By.XPATH, \"//div[@id='job_description']\")\n",
    "                description = description_elem.text.strip()\n",
    "                \n",
    "                # Extract company information\n",
    "                try:\n",
    "                    company_info_elem = driver.find_element(By.XPATH, \"//div[@class='company-info']//div[@data-cy='companyinfo-writeup']\")\n",
    "                    company_info = company_info_elem.text.strip()\n",
    "                except:\n",
    "                    company_info = \"\"\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                jobs.loc[len(jobs)] = [title, company, job_post_id, address, employment_type, seniority,\n",
    "                                        min_experience, job_category, salary, salary_type,\n",
    "                                        num_of_applications, last_posted_date, expiry_date, description, company_info]\n",
    "\n",
    "                driver.back()\n",
    "                sleep(3)\n",
    "\n",
    "            # Terminate the loop at the last page.\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "\n",
    "    jobs.to_csv(f'{search_term}_job.csv', index=False)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Read the list of search job terms\n",
    "search_terms_df = pd.read_csv('cf-search.csv')\n",
    "\n",
    "# Loop through each search job term and scrape jobs\n",
    "for index, row in search_terms_df.iterrows():\n",
    "    search_term = row['search']\n",
    "    print(f\"Scraping jobs for '{search_term}'\")\n",
    "    scrape_jobs(search_term)\n",
    "    print(f\"Finished scraping jobs for '{search_term}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4ace0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
